{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chardet\n",
    "\n",
    "# 文件批处理\n",
    "def FileReader(tag):\n",
    "    # print(os.getcwd())\n",
    "    path = r'./data/htl_del_4000/' + tag \n",
    "    os.listdir(path)\n",
    "    FileList = []\n",
    "    for iter in os.listdir(path):\n",
    "        if os.path.splitext(iter)[1] == '.txt':\n",
    "            FileList.append(iter)\n",
    "    return FileList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Windows\\Temp\\jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 对语料进行分词，去除停用词\n",
    "import jieba\n",
    "import json\n",
    "import re\n",
    "def _PreProcess(centence):\n",
    "    centence = re.sub('\\\\s','',centence)\n",
    "    centence = re.sub('[a-z,A-Z]*','',centence)\n",
    "    \n",
    "    row_list =  jieba.lcut(centence,cut_all = False)\n",
    "    # print(row_list)\n",
    "    return StripWord(row_list)\n",
    "def StripWord(text):\n",
    "    stop = open(r'./data/cn_stopwords.txt', 'r+', encoding='utf-8')\n",
    "    \n",
    "    tmp = stop.readlines()\n",
    "    tmp = ''.join(tmp)\n",
    "    stopWord = tmp.split(\"\\n\")\n",
    "    # print(stopWord[:100])\n",
    "    ans = []\n",
    "    for iter in text:\n",
    "        if not iter in stopWord:\n",
    "            # print(iter)\n",
    "            ans.append(iter)\n",
    "    return ans\n",
    "def PreProcess(path):\n",
    "    text = ''\n",
    "    # print(path)\n",
    "    with open(path,'r',encoding = 'gbk',errors = 'ignore') as txt:\n",
    "        text = txt.readline()\n",
    "    # print(text[:20])\n",
    "    return _PreProcess(text)\n",
    "\n",
    "\n",
    "def File_Dic(FileList,tag):\n",
    "    path = r'./data/htl_del_4000/' + tag + r'/'\n",
    "    dic = {}\n",
    "    for iter in range(0,len(FileList)):\n",
    "        cur_path = path + FileList[iter]\n",
    "        dic[iter] = PreProcess(cur_path)\n",
    "        # print(dic[iter])\n",
    "    return dic\n",
    "\n",
    "\n",
    "def row_split(tag):\n",
    "    FileList = FileReader(tag)\n",
    "    # ReWrite(FileList,tag)\n",
    "    volc = File_Dic(FileList,tag)\n",
    "    # print(len(FileList))\n",
    "    j_data = json.dumps(volc,ensure_ascii=False)\n",
    "    with open(r'./row_语料分词_' + tag + '.json' ,'w',encoding='utf-8') as f:\n",
    "        f.write(j_data)\n",
    "\n",
    "row_split('pos')\n",
    "row_split('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词表\n",
    "def vocabulary(path):\n",
    "    vocab = {}\n",
    "    ans = {}\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    for iter in vocab.keys():\n",
    "        ans = set(ans)|set(vocab[iter])\n",
    "    return ans\n",
    "\n",
    "def Construct_vocabu(Path,tag):\n",
    "    vocab = vocabulary(Path)\n",
    "    with open(r'词表_'+tag,'w',encoding = 'utf-8') as f:\n",
    "        for iter in vocab:\n",
    "            f.write(iter + '\\n')\n",
    "\n",
    "Construct_vocabu(r'./row_语料分词_neg.json','neg')\n",
    "Construct_vocabu(r'./row_语料分词_pos.json','pos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 总词表\n",
    "def merge_vocab(Path_pos,Path_neg):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    with open(Path_pos,'r',encoding='utf-8') as f:\n",
    "        pos = f.read().split('\\n')\n",
    "    with open(Path_neg,'r',encoding='utf-8') as f:\n",
    "        neg = f.read().split('\\n')\n",
    "    return set(pos)|set(neg)\n",
    "\n",
    "Merged_vocab = merge_vocab(r'./词表_pos',r'./词表_neg')\n",
    "\n",
    "with open(r'./词表_Merged','w',encoding='utf-8') as f:\n",
    "    for iter in Merged_vocab:\n",
    "        f.write(iter + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计文档频率\n",
    "import json\n",
    "def _Frequency(File_vocab,tag_vocab):\n",
    "    dic = {}\n",
    "    for iter in tag_vocab:\n",
    "        count = 0\n",
    "        for f in File_vocab.values():\n",
    "            if iter in f:\n",
    "                count += 1\n",
    "        dic[iter] = count\n",
    "    return dic\n",
    "def Frequency(File_Vocab_Path,Tag_Vocab_Path,tag):\n",
    "    File_vocab = {}\n",
    "    tag_vocab = []\n",
    "    with open(File_Vocab_Path,'r',encoding='utf-8') as f:\n",
    "        print()\n",
    "        File_vocab = json.load(f)\n",
    "    with open(Tag_Vocab_Path,'r',encoding='utf-8') as f:\n",
    "        tag_vocab = f.read()\n",
    "        tag_vocab = tag_vocab.split('\\n')\n",
    "    \n",
    "    dic = _Frequency(File_vocab,tag_vocab)\n",
    "    j_data = json.dumps(dic,ensure_ascii=False)\n",
    "    with open(r'./文档频率统计_' + tag + '.json','w',encoding = 'utf-8') as f:\n",
    "        f.write(j_data)\n",
    "    return\n",
    "def cal_Frequency(tag):\n",
    "    # print(r'./row_语料分词_'+tag +'.json')\n",
    "    Frequency(r'./row_语料分词_'+tag +'.json',r'./词表_Merged',tag)\n",
    "\n",
    "# dic1 = {'0':'[a,b,c]','1':'[d]'}\n",
    "# l = ['a','b','c']\n",
    "# ans = _Frequency(dic1,l)\n",
    "# print(type(ans))\n",
    "\n",
    "cal_Frequency('pos')\n",
    "cal_Frequency('neg')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Frequenc(Path_pos,Path_neg):\n",
    "    pos = {}\n",
    "    neg = {}\n",
    "    with open(Path_pos,'r',encoding = 'utf-8') as f:\n",
    "        pos = json.load(f)\n",
    "    with open(Path_neg,'r',encoding = 'utf-8') as f:\n",
    "        neg = json.load(f)\n",
    "    for iter in pos.keys():\n",
    "        neg[iter] += pos[iter]\n",
    "    return neg\n",
    "\n",
    "Merged_Frequ = merge_Frequenc(r'./文档频率统计_pos.json',r'./文档频率统计_neg.json')\n",
    "\n",
    "with open('./文档频率统计_merged.json','w',encoding = 'utf-8') as f:\n",
    "    tmp = json.dumps(Merged_Frequ,ensure_ascii=False)\n",
    "    f.write(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 200  201  201  202  204  205  209  212  220  231  231  232  244  245\n",
      "  250  253  254  254  260  263  263  265  277  285  300  309  310  354\n",
      "  379  391  393  403  413  418  468  490  524  526  548  551  606  632\n",
      "  689  766  820 1001 1012 1045 1870 2024]\n",
      "(15075,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n",
      "0\n",
      "15075\n"
     ]
    }
   ],
   "source": [
    "# 使用文档频率计算特征得分\n",
    "# 归一化看一下频率分布\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def _Frequency_distribute(freq):\n",
    "    \n",
    "    xpoints = np.arange(0,2000,2000/len(freq))\n",
    "    # print(np.shape(xpoints))\n",
    "    ypoints = np.array(freq)\n",
    "    print(np.shape(ypoints))\n",
    "\n",
    "    plt.plot(xpoints, ypoints)\n",
    "    plt.show()\n",
    "\n",
    "def Frequency_distribute(Path):\n",
    "    data = {}\n",
    "    with open(Path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    data_list = np.array(list(data.values()))\n",
    "    data_list.sort()\n",
    "\n",
    "    Max = data_list[-1]\n",
    "    Min = data_list[0]\n",
    "    print(data_list[-50:])\n",
    "    # Data_list = []\n",
    "    # for iter in data_list:\n",
    "    #     Data_list.append(float((iter - Min))/float((Max - Min)))\n",
    "    \n",
    "    _Frequency_distribute(data_list)\n",
    "    return Max,Min,len(data_list)\n",
    "\n",
    "Max,Min,Len = Frequency_distribute(r'./文档频率统计_merged.json')\n",
    "print (Max)\n",
    "print (Min)\n",
    "print (Len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据上面的分析设置阈值为 6-160\n",
    "# import json\n",
    "def _DF_result(Path):\n",
    "    data = {}\n",
    "    with open(Path,'r',encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    ans = [k for k,v in data.items() if v > 6 and v < 170] \n",
    "    return ans\n",
    "\n",
    "def DF_result():\n",
    "    data = _DF_result(r'./文档频率统计_merged.json')\n",
    "    path = r'./DF_特征词表'\n",
    "    with open (path,'w',encoding='utf-8') as f:\n",
    "        for iter in data:\n",
    "            # 去除单字词\n",
    "            if(len(iter) > 1):\n",
    "                f.write(iter + '\\n')\n",
    "\n",
    "DF_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于互信息\n",
    "# 计算一些统计量\n",
    "# import json\n",
    "import math\n",
    "neg_row = {}\n",
    "with open(r'./row_语料分词_neg.json','r',encoding = 'utf-8') as f:\n",
    "    neg_row = json.load(f)\n",
    "\n",
    "pos_row = {}\n",
    "with open(r'./row_语料分词_pos.json','r',encoding = 'utf-8') as f:\n",
    "    pos_row = json.load(f)\n",
    "\n",
    "file_num_neg = len(neg_row)\n",
    "file_num_pos = len(pos_row)\n",
    "\n",
    "merged = {}\n",
    "with open(r'./文档频率统计_merged.json','r',encoding = 'utf-8') as f:\n",
    "    merged = json.load(f)\n",
    "\n",
    "pos = {}\n",
    "with open(r'./文档频率统计_pos.json','r',encoding = 'utf-8') as f:\n",
    "    pos = json.load(f)\n",
    "\n",
    "neg = {}\n",
    "with open(r'./文档频率统计_neg.json','r',encoding = 'utf-8') as f:\n",
    "    neg = json.load(f)\n",
    "\n",
    "def cal_MI(A,B,C,N):\n",
    "    if(A*N == 0):\n",
    "        return 0\n",
    "    return math.log(float(A*N)/float((A+C))*(A+B))\n",
    "\n",
    "def pos_MI():\n",
    "    mi = {}\n",
    "    for iter in pos.keys():\n",
    "        mi[iter] = cal_MI(pos[iter],neg[iter],file_num_pos - pos[iter],file_num_neg + file_num_pos)\n",
    "    \n",
    "    return mi\n",
    "# pos MI\n",
    "def pos_MI():\n",
    "    mi = {}\n",
    "    for iter in pos.keys():\n",
    "        mi[iter] = cal_MI(pos[iter],neg[iter],file_num_pos - pos[iter],file_num_neg + file_num_pos)\n",
    "    \n",
    "    return mi\n",
    "# neg_MI\n",
    "def neg_MI():\n",
    "    mi = {}\n",
    "    for iter in neg.keys():\n",
    "        mi[iter] = cal_MI(neg[iter],pos[iter],file_num_neg - neg[iter],file_num_neg + file_num_pos)\n",
    "    \n",
    "    return mi\n",
    "\n",
    "pos_mi = pos_MI()\n",
    "neg_mi = neg_MI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取均值\n",
    "with open(r'./互信息_均值_score','w',encoding='utf-8') as f:\n",
    "    dic = {}\n",
    "    for iter in pos_mi.keys():\n",
    "        dic[iter] = '%.4f'%((pos_mi[iter] + neg_mi[iter])/2)\n",
    "    ord = dict(sorted(dic.items(), key = lambda kv:(kv[1], kv[0]),reverse=True))\n",
    "    data = list(ord.keys())[:1000]\n",
    "    # jdata = json.dumps(dic,ensure_ascii=False)\n",
    "    for iter in data:\n",
    "        f.write(iter + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加权\n",
    "with open(r'./互信息_概率加权_score','w',encoding='utf-8') as f:\n",
    "    dic = {}\n",
    "    for iter in pos_mi.keys():\n",
    "        dic[iter] = '%.4f'%((pos[iter] * pos_mi[iter] + neg[iter] * neg_mi[iter]) /(file_num_pos + file_num_neg))\n",
    "    \n",
    "    ord = dict(sorted(dic.items(), key = lambda kv:(kv[1], kv[0]),reverse=True))\n",
    "    data = list(ord.keys())[:1000]\n",
    "    # jdata = json.dumps(dic,ensure_ascii=False)\n",
    "    for iter in data:\n",
    "        f.write(iter + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于信息增益\n",
    "file_num_merge = file_num_pos +  file_num_neg\n",
    "def _entropy(A,B):\n",
    "    if B < 1e-6 and A < 1e-6:\n",
    "        return 0\n",
    "    if A < 1e-6:\n",
    "        return 0\n",
    "    if B < 1e-6:\n",
    "        return 20000\n",
    "    return float(A)/B * math.log(float(A)/B)\n",
    "def entropy(A):\n",
    "    return A* math.log(A)\n",
    "\n",
    "base1 = -1 * (_entropy( file_num_pos,file_num_merge) + _entropy(file_num_neg,file_num_merge))\n",
    "def IG(iter):\n",
    "    base2 = float(merged[iter])/file_num_merge * (_entropy(pos[iter],merged[iter]) + _entropy(neg[iter],merged[iter]))\n",
    "    base3 = (1-float(merged[iter])/file_num_merge) * (entropy(file_num_pos - pos[iter]) + entropy(file_num_pos - neg[iter]))\n",
    "    return base1 + base2 + base3\n",
    "\n",
    "def Mutual_info():\n",
    "    dic = {}\n",
    "    for iter in merged.keys():\n",
    "        if len(iter) > 1:\n",
    "            dic[iter] = '%.4f'%IG(iter)\n",
    "    \n",
    "    return dic\n",
    "\n",
    "with open(r'./互信息_信息增益_score','w',encoding='utf-8') as f:\n",
    "    \n",
    "    dic = Mutual_info()\n",
    "    ord = dict(sorted(dic.items(), key = lambda kv:(kv[1], kv[0]),reverse=True))\n",
    "    data = list(ord.keys())\n",
    "    for iter in data:\n",
    "        f.write(iter + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {'a':2,'n':0,'c':10}\n",
    "# ord = dict(sorted(dic.items(), key = lambda kv:(kv[1], kv[0]),reverse=True))\n",
    "# data = list(ord.keys())\n",
    "# print(ord)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex_6 利用TF_idf 构建文本的向量表示\n",
    "* 选择词表 互信息_信息增益_score\n",
    "* 之前的分词结果存储在 row_语料分词_tag 中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取分词后的文档，和 ex_5的词表,计算TF 与IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab_IG = []\n",
    "with open(r'./互信息_信息增益_score','r',encoding = 'utf-8') as f:\n",
    "    tmp = f.readline()\n",
    "    while tmp:\n",
    "        tmp = tmp.strip('\\n')\n",
    "        if tmp != '\\n':\n",
    "            Vocab_IG.append(tmp)\n",
    "        tmp = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['房间', '酒店', '龙门', '龙岩', '龙卡', '龙之梦', '龌龊', '齐齐哈尔', '齊齊', '鼾声', '鼻屎', '鼠患', '鼓风机', '鼓点', '鼓浪屿', '鼓吹', '鼓励', '鼓乐齐鸣', '鼎鼎有名', '黟县']\n"
     ]
    }
   ],
   "source": [
    "print(Vocab_IG[:20])\n",
    "# s = '\\n酒店\\n'\n",
    "# s.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有语料划分后的list\n",
    "import json\n",
    "import math\n",
    "L_row = []\n",
    "pos_row = {}\n",
    "neg_row = {}\n",
    "with open(r'./row_语料分词_pos.json','r',encoding='utf-8') as f:\n",
    "    pos_row = json.load(f)\n",
    "with open(r'./row_语料分词_neg.json','r',encoding='utf-8') as f:\n",
    "    neg_row = json.load(f)\n",
    "\n",
    "for iter in pos_row.values():\n",
    "    L_row.append(iter)\n",
    "for iter in neg_row.values():\n",
    "    L_row.append(iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算TF-idf\n",
    "# Vector_TF = []\n",
    "Vector_TF_IDF = []\n",
    "for article in L_row:\n",
    "    # tmp_TF = []\n",
    "    tmp_TF_IDF = []\n",
    "    for iter in Vocab_IG:\n",
    "        if iter in article:\n",
    "            tmp_TF_IDF.append(float(article.count(iter))/len(article) * math.log(float(file_num_merge)/merged[iter]))\n",
    "        else:\n",
    "            tmp_TF_IDF.append(0)\n",
    "            \n",
    "    Vector_TF_IDF.append(tmp_TF_IDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Vector_TF_IDF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把IDF 存储到文件\n",
    "with open(r'./文本特征词表_TF_IDF','w',encoding='utf-8') as f:\n",
    "    for i in range(0,len(Vector_TF_IDF)):\n",
    "        f.write(str(i) + '\\t' + str(Vector_TF_IDF[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pickle 保存list\n",
    "import pickle\n",
    "with open(r'./序列化文本特征词表_TF_IDF','wb') as f:\n",
    "    pickle.dump(Vector_TF_IDF,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于余弦相似度的文本相似度度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文档的向量表示\n",
    "# Vector_TF_IDF = []\n",
    "# with open(r'./序列化文本特征词表_TF_IDF','rb') as f:\n",
    "#     Vector_TF_IDF =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "def Cos_similar(A,B):\n",
    "    # A = np.array(l1)\n",
    "    # B = np.array(l1)\n",
    "    return float(np.dot(A,B))/(np.linalg.norm(A) * np.linalg.norm(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算整个数据集中相似度最高的三篇文档\n",
    "# import multiprocessing \n",
    "# def func(Vector_TF_IDF,iter,Sim):\n",
    "    \n",
    "#     for oth in range(0,len(Vector_TF_IDF)):\n",
    "#         if oth == iter:\n",
    "#             continue\n",
    "#         Sim.append(Cos_similar(Vector_TF_IDF[iter],Vector_TF_IDF[oth]))\n",
    "\n",
    "# manager = multiprocessing.Manager()\n",
    "# pool=multiprocessing.Pool(4)\n",
    "Sim = []\n",
    "# Sim=manager.list()\n",
    "for iter in range(0,len(Vector_TF_IDF)):\n",
    "    # pool.apply_async(func,args=(Vector_TF_IDF,iter,Sim,))    \n",
    "    for oth in range(0,len(Vector_TF_IDF)):\n",
    "        if oth == iter:\n",
    "            continue\n",
    "        Sim.append(Cos_similar(Vector_TF_IDF[iter],Vector_TF_IDF[oth]))\n",
    "\n",
    "# pool.close()\n",
    "# pool.join()\n",
    "Sim.sort(reverse=True)\n",
    "print(Sim[:3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b26237669fb82f55901c81719d6b1a1c6f49e3c8345e7550e9986512a5b1f1dc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
